{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "869c4a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale\n",
    "from os.path import join\n",
    "from sklearn.metrics import accuracy_score as accuracy, f1_score, mean_absolute_error as mae\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
    "from pathlib2 import Path\n",
    "from tensorflow.keras import backend as K, callbacks\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c486ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_dir = ''\n",
    "TRAIN_ROOT_PATH = join(Base_dir, 'Dataset')\n",
    "train_file_names = os.listdir(join(Base_dir, 'Dataset'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0096e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Processed_DJI.csv',\n",
       " 'Processed_NASDAQ.csv',\n",
       " 'Processed_NYSE.csv',\n",
       " 'Processed_RUSSELL.csv',\n",
       " 'Processed_SP.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b387c7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data ...\n"
     ]
    }
   ],
   "source": [
    "# if moving average = 0 then we have no moving average\n",
    "seq_len = 60\n",
    "moving_average_day = 0\n",
    "number_of_stocks = 0\n",
    "number_feature = 0\n",
    "samples_in_each_stock = 0\n",
    "number_filter = [8, 8, 8]\n",
    "predict_day = 1\n",
    "\n",
    "cnn_train_data, cnn_train_target, cnn_test_data, cnn_test_target, cnn_valid_data, cnn_valid_target = ([] for i in\n",
    "                                                                                                      range(6))\n",
    "\n",
    "print('Loading train data ...')\n",
    "order_stocks = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "648587b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_fir):\n",
    "    try:\n",
    "        df_raw = pd.read_csv(file_fir, index_col='Date') # parse_dates=['Date'])\n",
    "    except IOError:\n",
    "        print(\"IO ERROR\")\n",
    "    return df_raw\n",
    "\n",
    "def costruct_data_warehouse(ROOT_PATH, file_names):\n",
    "    global number_of_stocks\n",
    "    global samples_in_each_stock\n",
    "    global number_feature\n",
    "    global order_stocks\n",
    "    data_warehouse = {}\n",
    "\n",
    "    for stock_file_name in file_names:\n",
    "\n",
    "        file_dir = os.path.join(ROOT_PATH, stock_file_name)\n",
    "        ## Loading Data\n",
    "        try:\n",
    "            df_raw = load_data(file_dir)\n",
    "        except ValueError:\n",
    "            print(\"Couldn't Read {} file\".format(file_dir))\n",
    "\n",
    "        number_of_stocks += 1\n",
    "\n",
    "        data = df_raw\n",
    "        df_name = data['Name'][0]\n",
    "        order_stocks.append(df_name)\n",
    "        del data['Name']\n",
    "\n",
    "        target = (data['Close'][predict_day:] / data['Close'][:-predict_day].values).astype(int)\n",
    "        data = data[:-predict_day]\n",
    "        target.index = data.index\n",
    "        # Becasue of using 200 days Moving Average as one of the features\n",
    "        data = data[200:]\n",
    "        data = data.fillna(0)\n",
    "        data['target'] = target\n",
    "        target = data['target']\n",
    "        # data['Date'] = data['Date'].apply(lambda x: x.weekday())\n",
    "        del data['target']\n",
    "\n",
    "        number_feature = data.shape[1]\n",
    "        samples_in_each_stock = data.shape[0]\n",
    "\n",
    "        train_data = data[data.index < '2016-04-21']\n",
    "        train_data1 = scale(train_data)\n",
    "        # print train_data.shape\n",
    "        train_target1 = target[target.index < '2016-04-21']\n",
    "        train_data = train_data1[:int(0.75 * train_data1.shape[0])]\n",
    "        train_target = train_target1[:int(0.75 * train_target1.shape[0])]\n",
    "\n",
    "        valid_data = scale(train_data1[int(0.75 * train_data1.shape[0]) - seq_len:])\n",
    "        valid_target = train_target1[int(0.75 * train_target1.shape[0]) - seq_len:]\n",
    "\n",
    "        data = pd.DataFrame(scale(data.values), columns=data.columns)\n",
    "        data.index = target.index\n",
    "        test_data = data[data.index >= '2016-04-21']\n",
    "        test_target = target[target.index >= '2016-04-21']\n",
    "\n",
    "        data_warehouse[df_name] = [train_data, train_target, np.array(test_data), np.array(test_target), valid_data,\n",
    "                                   valid_target]\n",
    "\n",
    "    return data_warehouse\n",
    "\n",
    "def sklearn_acc(model, test_data, test_target):\n",
    "    overall_results = model.predict(test_data)\n",
    "    test_pred = (overall_results > 0.5).astype(int)\n",
    "    acc_results = [mae(overall_results, test_target), accuracy(test_pred, test_target),\n",
    "                   f1_score(test_pred, test_target, average='macro')]\n",
    "\n",
    "    return acc_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d5685ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_warehouse = costruct_data_warehouse(TRAIN_ROOT_PATH, train_file_names)\n",
    "# order_stocks = data_warehouse.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26ad501c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DJI', 'NASDAQ', 'NYA', 'RUT', 'S&P'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_warehouse.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "610ab024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stocks = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('number of stocks = '), number_of_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0225aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_warehouse, i):\n",
    "    seq_len = 60\n",
    "    epochs = 200\n",
    "    drop = 0.1\n",
    "\n",
    "    global cnn_train_data, cnn_train_target, cnn_test_data, cnn_test_target, cnn_valid_data, cnn_valid_target\n",
    "\n",
    "    if i == 1:\n",
    "        print('sequencing ...')\n",
    "        cnn_train_data, cnn_train_target, cnn_test_data, cnn_test_target, cnn_valid_data, cnn_valid_target = cnn_data_sequence(\n",
    "            data_warehouse, seq_len)\n",
    "\n",
    "    my_file = Path(join(Base_dir,\n",
    "        '2D-models/best-{}-{}-{}-{}-{}.h5'.format(epochs, seq_len, number_filter, drop, i)))\n",
    "    filepath = join(Base_dir, '2D-models/best-{}-{}-{}-{}-{}.h5'.format(epochs, seq_len, number_filter, drop, i))\n",
    "    if my_file.is_file():\n",
    "        print('loading model')\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(' fitting model to target')\n",
    "        model = Sequential()\n",
    "        #\n",
    "        # layer 1\n",
    "        model.add(\n",
    "            Conv2D(number_filter[0], (1, number_feature), activation='relu', input_shape=(seq_len, number_feature, 1)))\n",
    "        # layer 2\n",
    "        model.add(Conv2D(number_filter[1], (3, 1), activation='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2, 1)))\n",
    "\n",
    "        # layer 3\n",
    "        model.add(Conv2D(number_filter[2], (3, 1), activation='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2, 1)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(drop))\n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.compile(optimizer='Adam', loss='mae', metrics=['acc', f1])\n",
    "\n",
    "        best_model = callbacks.ModelCheckpoint(filepath, monitor='val_f1', verbose=0, save_best_only=True,\n",
    "                                               save_weights_only=False, mode='max', period=1)\n",
    "\n",
    "\n",
    "        model.fit(cnn_train_data, cnn_train_target, epochs=epochs, batch_size=128, verbose=1,\n",
    "                        validation_data=(cnn_valid_data, cnn_valid_target), callbacks=[best_model])\n",
    "    model = load_model(filepath, custom_objects={'f1': f1})\n",
    "\n",
    "    return model, seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5be2d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_data_sequence_pre_train(data, target, seque_len):\n",
    "    new_data = []\n",
    "    new_target = []\n",
    "    for index in range(data.shape[0] - seque_len + 1):\n",
    "        new_data.append(data[index: index + seque_len])\n",
    "        new_target.append(target[index + seque_len - 1])\n",
    "\n",
    "    new_data = np.array(new_data)\n",
    "    new_target = np.array(new_target)\n",
    "\n",
    "    new_data = new_data.reshape(new_data.shape[0], new_data.shape[1], new_data.shape[2], 1)\n",
    "\n",
    "    return new_data, new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05d58472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(data_warehouse, model, seque_len, order_stocks, cnn_results):\n",
    "    for name in order_stocks:\n",
    "        value = data_warehouse[name]\n",
    "        # train_data, train_target = cnn_data_sequence_pre_train(value[0], value[1], seque_len)\n",
    "        test_data, test_target = cnn_data_sequence_pre_train(value[2], value[3], seque_len)\n",
    "        # valid_data, valid_target = cnn_data_sequence_pre_train(value[4], value[5], seque_len)\n",
    "\n",
    "        cnn_results.append(sklearn_acc(model, test_data, test_target)[2])\n",
    "\n",
    "    return cnn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0bc985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn_ann(data_warehouse, order_stocks):\n",
    "    cnn_results = []\n",
    "    # dnn_results = []\n",
    "    iterate_no = 4\n",
    "    for i in range(1, iterate_no):\n",
    "        K.clear_session()\n",
    "        print(i)\n",
    "        model, seq_len = train(data_warehouse, i)\n",
    "        # cnn_results, dnn_results = prediction(data_warehouse, model, seq_len, order_stocks, cnn_results)\n",
    "        cnn_results = prediction(data_warehouse, model, seq_len, order_stocks, cnn_results)\n",
    "\n",
    "    cnn_results = np.array(cnn_results)\n",
    "    cnn_results = cnn_results.reshape(iterate_no - 1, len(order_stocks))\n",
    "    cnn_results = pd.DataFrame(cnn_results, columns=order_stocks)\n",
    "#   cnn_results = cnn_results.append([cnn_results.mean(), cnn_results.max(), cnn_results.std()], ignore_index=True)\n",
    "    return cnn_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30e1d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_data_sequence(data_warehouse, seq_len):\n",
    "    tottal_train_data = []\n",
    "    tottal_train_target = []\n",
    "    tottal_valid_data = []\n",
    "    tottal_valid_target = []\n",
    "    tottal_test_data = []\n",
    "    tottal_test_target = []\n",
    "\n",
    "    for key, value in data_warehouse.items():\n",
    "        tottal_train_data, tottal_train_target = cnn_data_sequence_separately(tottal_train_data, tottal_train_target,\n",
    "                                                                              value[0], value[1], seq_len)\n",
    "        tottal_test_data, tottal_test_target = cnn_data_sequence_separately(tottal_test_data, tottal_test_target,\n",
    "                                                                            value[2], value[3], seq_len)\n",
    "        tottal_valid_data, tottal_valid_target = cnn_data_sequence_separately(tottal_valid_data, tottal_valid_target,\n",
    "                                                                              value[4], value[5], seq_len)\n",
    "\n",
    "    tottal_train_data = np.array(tottal_train_data)\n",
    "    tottal_train_target = np.array(tottal_train_target)\n",
    "    tottal_test_data = np.array(tottal_test_data)\n",
    "    tottal_test_target = np.array(tottal_test_target)\n",
    "    tottal_valid_data = np.array(tottal_valid_data)\n",
    "    tottal_valid_target = np.array(tottal_valid_target)\n",
    "\n",
    "    tottal_train_data = tottal_train_data.reshape(tottal_train_data.shape[0], tottal_train_data.shape[1],\n",
    "                                                  tottal_train_data.shape[2], 1)\n",
    "    tottal_test_data = tottal_test_data.reshape(tottal_test_data.shape[0], tottal_test_data.shape[1],\n",
    "                                                tottal_test_data.shape[2], 1)\n",
    "    tottal_valid_data = tottal_valid_data.reshape(tottal_valid_data.shape[0], tottal_valid_data.shape[1],\n",
    "                                                  tottal_valid_data.shape[2], 1)\n",
    "\n",
    "    return tottal_train_data, tottal_train_target, tottal_test_data, tottal_test_target, tottal_valid_data, tottal_valid_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4415d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision_pos = precision(y_true, y_pred)\n",
    "    recall_pos = recall(y_true, y_pred)\n",
    "    precision_neg = precision((K.ones_like(y_true) - y_true), (K.ones_like(y_pred) - K.clip(y_pred, 0, 1)))\n",
    "    recall_neg = recall((K.ones_like(y_true) - y_true), (K.ones_like(y_pred) - K.clip(y_pred, 0, 1)))\n",
    "    f_posit = 2 * ((precision_pos * recall_pos) / (precision_pos + recall_pos + K.epsilon()))\n",
    "    f_neg = 2 * ((precision_neg * recall_neg) / (precision_neg + recall_neg + K.epsilon()))\n",
    "\n",
    "    return (f_posit + f_neg) / 2\n",
    "\n",
    "\n",
    "def load_data(file_fir):\n",
    "    try:\n",
    "        df_raw = pd.read_csv(file_fir, index_col='Date') # parse_dates=['Date'])\n",
    "    except IOError:\n",
    "        print(\"IO ERROR\")\n",
    "    return df_raw\n",
    "\n",
    "\n",
    "def costruct_data_warehouse(ROOT_PATH, file_names):\n",
    "    global number_of_stocks\n",
    "    global samples_in_each_stock\n",
    "    global number_feature\n",
    "    global order_stocks\n",
    "    data_warehouse = {}\n",
    "\n",
    "    for stock_file_name in file_names:\n",
    "\n",
    "        file_dir = os.path.join(ROOT_PATH, stock_file_name)\n",
    "        ## Loading Data\n",
    "        try:\n",
    "            df_raw = load_data(file_dir)\n",
    "        except ValueError:\n",
    "            print(\"Couldn't Read {} file\".format(file_dir))\n",
    "\n",
    "        number_of_stocks += 1\n",
    "\n",
    "        data = df_raw\n",
    "        df_name = data['Name'][0]\n",
    "        order_stocks.append(df_name)\n",
    "        del data['Name']\n",
    "\n",
    "        target = (data['Close'][predict_day:] / data['Close'][:-predict_day].values).astype(int)\n",
    "        data = data[:-predict_day]\n",
    "        target.index = data.index\n",
    "        # Becasue of using 200 days Moving Average as one of the features\n",
    "        data = data[200:]\n",
    "        data = data.fillna(0)\n",
    "        data['target'] = target\n",
    "        target = data['target']\n",
    "        # data['Date'] = data['Date'].apply(lambda x: x.weekday())\n",
    "        del data['target']\n",
    "\n",
    "        number_feature = data.shape[1]\n",
    "        samples_in_each_stock = data.shape[0]\n",
    "\n",
    "        train_data = data[data.index < '2016-04-21']\n",
    "        train_data1 = scale(train_data)\n",
    "        # print train_data.shape\n",
    "        train_target1 = target[target.index < '2016-04-21']\n",
    "        train_data = train_data1[:int(0.75 * train_data1.shape[0])]\n",
    "        train_target = train_target1[:int(0.75 * train_target1.shape[0])]\n",
    "\n",
    "        valid_data = scale(train_data1[int(0.75 * train_data1.shape[0]) - seq_len:])\n",
    "        valid_target = train_target1[int(0.75 * train_target1.shape[0]) - seq_len:]\n",
    "\n",
    "        data = pd.DataFrame(scale(data.values), columns=data.columns)\n",
    "        data.index = target.index\n",
    "        test_data = data[data.index >= '2016-04-21']\n",
    "        test_target = target[target.index >= '2016-04-21']\n",
    "\n",
    "        data_warehouse[df_name] = [train_data, train_target, np.array(test_data), np.array(test_target), valid_data,\n",
    "                                   valid_target]\n",
    "\n",
    "    return data_warehouse\n",
    "\n",
    "\n",
    "def cnn_data_sequence_separately(tottal_data, tottal_target, data, target, seque_len):\n",
    "    for index in range(data.shape[0] - seque_len + 1):\n",
    "        tottal_data.append(data[index: index + seque_len])\n",
    "        tottal_target.append(target[index + seque_len - 1])\n",
    "\n",
    "    return tottal_data, tottal_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fb79975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "sequencing ...\n",
      "loading model\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 3ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 997us/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "2\n",
      "loading model\n",
      "11/11 [==============================] - 0s 823us/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 0s/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "3\n",
      "loading model\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 3ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn_results = run_cnn_ann(data_warehouse, order_stocks)\n",
    "\n",
    "\n",
    "results = pd.concat([cnn_results, pd.DataFrame([cnn_results.mean(), cnn_results.max(), cnn_results.std()])], ignore_index=True)\n",
    "results.to_csv(join(Base_dir, '2D-models/new results.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
